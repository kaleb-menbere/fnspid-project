{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65cc756",
   "metadata": {},
   "source": [
    "## Task 1: Git and GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE FINANCIAL NEWS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "print(\"\\nüìÅ 1. LOADING AND PREPARING DATA...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load your data\n",
    "news_df = pd.read_csv(\"../data/raw_analyst_ratings.csv\")\n",
    "\n",
    "print(f\"Original data shape: {news_df.shape}\")\n",
    "print(f\"Columns: {news_df.columns.tolist()}\")\n",
    "\n",
    "# Convert date to datetime and set as index - FIXED VERSION\n",
    "print(\"Converting date format and setting index...\")\n",
    "\n",
    "# Remove timezone info from dates that have it, then parse\n",
    "news_df['date'] = pd.to_datetime(\n",
    "    news_df['date'].str.replace(r'[-+]\\d{2}:\\d{2}$', '', regex=True), \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Remove any truly invalid dates (should be very few now)\n",
    "initial_count = len(news_df)\n",
    "news_df = news_df.dropna(subset=['date'])\n",
    "print(f\"Removed {initial_count - len(news_df)} rows with invalid dates\")\n",
    "\n",
    "news_df.set_index('date', inplace=True)\n",
    "\n",
    "# Add additional time-based features (convert to integers to avoid float issues)\n",
    "news_df['day_of_week'] = news_df.index.day_name()\n",
    "news_df['hour'] = news_df.index.hour.astype(int)  # Convert to integer\n",
    "news_df['month'] = news_df.index.month.astype(int)  # Convert to integer\n",
    "news_df['year'] = news_df.index.year.astype(int)  # Convert to integer\n",
    "news_df['headline_length'] = news_df['headline'].str.len()\n",
    "\n",
    "print(f\"‚úÖ Data prepared: {len(news_df):,} articles from {news_df.index.min().strftime('%Y-%m-%d')} to {news_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DESCRIPTIVE STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüìä 2. DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n2.1 TEXTUAL LENGTH STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "text_stats = news_df['headline_length'].describe()\n",
    "print(f\"Headline Length Statistics:\")\n",
    "print(f\"‚Ä¢ Count: {text_stats['count']:,}\")\n",
    "print(f\"‚Ä¢ Mean: {text_stats['mean']:.1f} characters\")\n",
    "print(f\"‚Ä¢ Std: {text_stats['std']:.1f}\")\n",
    "print(f\"‚Ä¢ Min: {text_stats['min']} characters\")\n",
    "print(f\"‚Ä¢ 25%: {text_stats['25%']:.1f} characters\")\n",
    "print(f\"‚Ä¢ 50%: {text_stats['50%']:.1f} characters\")\n",
    "print(f\"‚Ä¢ 75%: {text_stats['75%']:.1f} characters\")\n",
    "print(f\"‚Ä¢ Max: {text_stats['max']} characters\")\n",
    "\n",
    "# Plot headline length distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(news_df['headline_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(text_stats['mean'], color='red', linestyle='--', label=f'Mean: {text_stats[\"mean\"]:.1f}')\n",
    "plt.xlabel('Headline Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Headline Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "news_df['headline_length'].plot(kind='box')\n",
    "plt.title('Box Plot of Headline Lengths')\n",
    "plt.ylabel('Characters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n2.2 PUBLISHER ACTIVITY ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "publisher_counts = news_df['publisher'].value_counts()\n",
    "print(f\"Total unique publishers: {len(publisher_counts):,}\")\n",
    "\n",
    "print(f\"\\nTop 15 Most Active Publishers:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (publisher, count) in enumerate(publisher_counts.head(15).items(), 1):\n",
    "    percentage = (count / len(news_df)) * 100\n",
    "    print(f\"{i:2d}. {publisher:<40} {count:>8,} articles ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Plot publisher activity\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20_publishers = publisher_counts.head(20)\n",
    "plt.barh(range(len(top_20_publishers)), top_20_publishers.values)\n",
    "plt.yticks(range(len(top_20_publishers)), top_20_publishers.index)\n",
    "plt.xlabel('Number of Articles')\n",
    "plt.title('Top 20 Publishers by Article Volume')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n2.3 TEMPORAL PUBLICATION PATTERNS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Day of week analysis\n",
    "day_counts = news_df['day_of_week'].value_counts()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = day_counts.reindex(day_order)\n",
    "\n",
    "print(\"\\nArticles by Day of Week:\")\n",
    "for day, count in day_counts.items():\n",
    "    percentage = (count / len(news_df)) * 100\n",
    "    print(f\"‚Ä¢ {day:<12}: {count:>8,} articles ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Hour of day analysis\n",
    "hour_counts = news_df['hour'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nArticles by Hour of Day (Top 5):\")\n",
    "for hour, count in hour_counts.nlargest(5).items():\n",
    "    percentage = (count / len(news_df)) * 100\n",
    "    hour_int = int(hour)  # Convert to integer for formatting\n",
    "    next_hour = (hour_int + 1) % 24\n",
    "    print(f\"‚Ä¢ {hour_int:02d}:00 - {next_hour:02d}:00: {count:>6,} articles ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Plot temporal patterns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Day of week plot\n",
    "ax1.bar(day_counts.index, day_counts.values, color='lightblue', edgecolor='navy')\n",
    "ax1.set_title('Article Publication by Day of Week')\n",
    "ax1.set_xlabel('Day of Week')\n",
    "ax1.set_ylabel('Number of Articles')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Hour of day plot\n",
    "hour_counts_index = [int(h) for h in hour_counts.index]  # Convert to integers\n",
    "ax2.bar(hour_counts_index, hour_counts.values, color='lightcoral', edgecolor='darkred')\n",
    "ax2.set_title('Article Publication by Hour of Day')\n",
    "ax2.set_xlabel('Hour of Day (24h)')\n",
    "ax2.set_ylabel('Number of Articles')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(0, 24, 2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TEXT ANALYSIS (TOPIC MODELING)\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüî§ 3. TEXT ANALYSIS - TOPIC MODELING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n3.1 KEYWORD AND PHRASE EXTRACTION\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Define financial keywords and topics to search for\n",
    "financial_keywords = {\n",
    "    'earnings': ['earnings', 'profit', 'revenue', 'eps', 'quarterly results'],\n",
    "    'price_targets': ['price target', 'target price', 'raised to', 'lowered to', 'maintained at'],\n",
    "    'analyst_ratings': ['upgrade', 'downgrade', 'initiate coverage', 'maintain', 'buy', 'sell', 'hold'],\n",
    "    'fda_approvals': ['fda approval', 'fda clears', 'regulatory approval', 'clinical trial'],\n",
    "    'mergers_acquisitions': ['merger', 'acquisition', 'takeover', 'buyout', 'acquires'],\n",
    "    'stock_movements': ['stock up', 'stock down', 'surges', 'plunges', 'jumps', 'falls'],\n",
    "    'dividends': ['dividend', 'payout', 'yield', 'dividend increase']\n",
    "}\n",
    "\n",
    "def count_keyword_occurrences(text, keywords):\n",
    "    \"\"\"Count occurrences of keywords in text (case insensitive)\"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    return sum(1 for keyword in keywords if keyword in text_lower)\n",
    "\n",
    "# Count occurrences for each category\n",
    "keyword_counts = {}\n",
    "for category, keywords in financial_keywords.items():\n",
    "    count = news_df['headline'].apply(lambda x: count_keyword_occurrences(x, keywords)).sum()\n",
    "    keyword_counts[category] = count\n",
    "\n",
    "print(\"Most Common Financial Topics in Headlines:\")\n",
    "print(\"-\" * 50)\n",
    "for category, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(news_df)) * 100\n",
    "    print(f\"‚Ä¢ {category.replace('_', ' ').title():<20}: {count:>6,} occurrences ({percentage:>4.1f}%)\")\n",
    "\n",
    "# Plot keyword frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories = list(keyword_counts.keys())\n",
    "counts = list(keyword_counts.values())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "\n",
    "bars = plt.bar(range(len(categories)), counts, color=colors, edgecolor='black')\n",
    "plt.xticks(range(len(categories)), [cat.replace('_', '\\n').title() for cat in categories], rotation=45)\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.title('Financial Topic Frequency in News Headlines')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01, \n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n3.2 SPECIFIC FINANCIAL TERM ANALYSIS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Analyze specific important terms in detail\n",
    "important_terms = ['FDA', 'upgrade', 'downgrade', 'price target', 'earnings', 'dividend']\n",
    "\n",
    "term_counts = {}\n",
    "for term in important_terms:\n",
    "    count = news_df['headline'].str.contains(term, case=False, na=False).sum()\n",
    "    term_counts[term] = count\n",
    "\n",
    "print(\"Specific Financial Term Analysis:\")\n",
    "for term, count in sorted(term_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(news_df)) * 100\n",
    "    print(f\"‚Ä¢ '{term:<12}': {count:>6,} occurrences ({percentage:>4.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TIME SERIES ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüìà 4. TIME SERIES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n4.1 PUBLICATION FREQUENCY OVER TIME\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Resample at different frequencies\n",
    "daily_counts = news_df.resample('D').size()\n",
    "weekly_counts = news_df.resample('W').size()\n",
    "monthly_counts = news_df.resample('M').size()\n",
    "\n",
    "print(f\"Time Series Statistics:\")\n",
    "print(f\"‚Ä¢ Daily average: {daily_counts.mean():.1f} articles\")\n",
    "print(f\"‚Ä¢ Daily std: {daily_counts.std():.1f}\")\n",
    "print(f\"‚Ä¢ Maximum daily articles: {daily_counts.max()} on {daily_counts.idxmax().strftime('%Y-%m-%d')}\")\n",
    "print(f\"‚Ä¢ Minimum daily articles: {daily_counts.min()} on {daily_counts.idxmin().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Plot time series at different frequencies\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Daily plot\n",
    "ax1.plot(daily_counts.index, daily_counts.values, color='green', linewidth=1, alpha=0.7)\n",
    "ax1.set_title('Articles Published Per Day')\n",
    "ax1.set_ylabel('Number of Articles')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.fill_between(daily_counts.index, daily_counts.values, alpha=0.3, color='green')\n",
    "\n",
    "# Weekly plot\n",
    "ax2.plot(weekly_counts.index, weekly_counts.values, color='blue', linewidth=2)\n",
    "ax2.set_title('Articles Published Per Week')\n",
    "ax2.set_ylabel('Number of Articles')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly plot\n",
    "ax3.plot(monthly_counts.index, monthly_counts.values, color='purple', linewidth=2)\n",
    "ax3.set_title('Articles Published Per Month')\n",
    "ax3.set_ylabel('Number of Articles')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n4.2 VOLATILITY AND SPIKES ANALYSIS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Calculate rolling statistics to identify volatility\n",
    "weekly_rolling = weekly_counts.rolling(window=4).mean()  # 4-week rolling average\n",
    "\n",
    "# Identify significant spikes (more than 2 standard deviations from mean)\n",
    "spike_threshold = weekly_counts.mean() + 2 * weekly_counts.std()\n",
    "significant_spikes = weekly_counts[weekly_counts > spike_threshold]\n",
    "\n",
    "print(f\"Significant publication spikes (> {spike_threshold:.1f} articles/week):\")\n",
    "print(\"-\" * 60)\n",
    "for date, count in significant_spikes.items():\n",
    "    print(f\"‚Ä¢ {date.strftime('%Y-%m-%d')}: {count} articles\")\n",
    "\n",
    "# Plot with spikes highlighted\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(weekly_counts.index, weekly_counts.values, color='blue', linewidth=2, label='Weekly Articles')\n",
    "plt.plot(weekly_rolling.index, weekly_rolling.values, color='red', linewidth=2, linestyle='--', label='4-Week Moving Average')\n",
    "plt.axhline(y=spike_threshold, color='orange', linestyle=':', label=f'Spike Threshold ({spike_threshold:.0f})')\n",
    "\n",
    "# Highlight spikes\n",
    "spike_dates = significant_spikes.index\n",
    "spike_values = significant_spikes.values\n",
    "plt.scatter(spike_dates, spike_values, color='red', s=50, zorder=5, label='Significant Spikes')\n",
    "\n",
    "plt.title('Weekly Publication Frequency with Spike Detection')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n4.3 SEASONAL PATTERNS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Analyze seasonal patterns by month - FIXED: Ensure month is integer\n",
    "monthly_avg = news_df.groupby('month').size()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "print(\"Average Articles by Month:\")\n",
    "for month, count in monthly_avg.items():\n",
    "    month_int = int(month)  # Convert to integer for indexing\n",
    "    print(f\"‚Ä¢ {month_names[month_int-1]:<3}: {count:>6,} articles\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Use the integer months for positioning\n",
    "months_sorted = sorted(monthly_avg.index)\n",
    "values_sorted = [monthly_avg[month] for month in months_sorted]\n",
    "month_labels = [month_names[int(month)-1] for month in months_sorted]\n",
    "\n",
    "plt.bar(month_labels, values_sorted, color='lightseagreen', edgecolor='darkgreen')\n",
    "plt.title('Average Article Publication by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. PUBLISHER ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüè¢ 5. PUBLISHER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n5.1 PUBLISHER CONTRIBUTION ANALYSIS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Calculate market share metrics\n",
    "total_articles = len(news_df)\n",
    "top_10_publishers = publisher_counts.head(10)\n",
    "top_10_share = (top_10_publishers.sum() / total_articles) * 100\n",
    "\n",
    "print(f\"Market Concentration Analysis:\")\n",
    "print(f\"‚Ä¢ Top 10 publishers account for {top_10_share:.1f}% of all articles\")\n",
    "print(f\"‚Ä¢ Remaining {len(publisher_counts) - 10} publishers share {100 - top_10_share:.1f}%\")\n",
    "\n",
    "# Cumulative distribution\n",
    "cumulative_percentage = (publisher_counts.cumsum() / total_articles * 100)\n",
    "\n",
    "print(f\"\\nPublisher Reach Analysis:\")\n",
    "print(f\"‚Ä¢ Top 5 publishers: {cumulative_percentage.iloc[4]:.1f}% of articles\")\n",
    "print(f\"‚Ä¢ Top 20 publishers: {cumulative_percentage.iloc[19]:.1f}% of articles\")\n",
    "print(f\"‚Ä¢ Top 50 publishers: {cumulative_percentage.iloc[49]:.1f}% of articles\")\n",
    "\n",
    "print(\"\\n5.2 CONTENT CHARACTERISTICS BY PUBLISHER\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyze publisher content patterns\n",
    "publisher_analysis = news_df.groupby('publisher').agg({\n",
    "    'headline_length': ['mean', 'std'],\n",
    "    'stock': 'nunique',\n",
    "    'hour': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "publisher_analysis.columns = ['avg_headline_len', 'std_headline_len', 'unique_stocks', 'avg_hour', 'std_hour']\n",
    "publisher_analysis['total_articles'] = publisher_counts\n",
    "\n",
    "print(\"\\nTop Publishers - Content Characteristics:\")\n",
    "print(\"-\" * 55)\n",
    "top_publishers_stats = publisher_analysis.nlargest(10, 'total_articles')[\n",
    "    ['total_articles', 'unique_stocks', 'avg_headline_len', 'avg_hour']\n",
    "]\n",
    "print(top_publishers_stats.to_string())\n",
    "\n",
    "print(\"\\n5.3 EMAIL DOMAIN ANALYSIS\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "def extract_email_domain(publisher_name):\n",
    "    \"\"\"Extract domain from email addresses in publisher names\"\"\"\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    matches = re.findall(email_pattern, str(publisher_name))\n",
    "    if matches:\n",
    "        return matches[0].split('@')[1]\n",
    "    return None\n",
    "\n",
    "# Extract domains\n",
    "news_df['publisher_domain'] = news_df['publisher'].apply(extract_email_domain)\n",
    "domain_analysis = news_df[news_df['publisher_domain'].notna()]\n",
    "\n",
    "if not domain_analysis.empty:\n",
    "    domain_counts = domain_analysis['publisher_domain'].value_counts()\n",
    "    \n",
    "    print(f\"Email Domain Analysis Results:\")\n",
    "    print(f\"‚Ä¢ Articles from email-based publishers: {len(domain_analysis):,}\")\n",
    "    print(f\"‚Ä¢ Unique domains identified: {domain_analysis['publisher_domain'].nunique()}\")\n",
    "    \n",
    "    print(f\"\\nTop Email Domains:\")\n",
    "    for domain, count in domain_counts.head(10).items():\n",
    "        percentage = (count / len(domain_analysis)) * 100\n",
    "        print(f\"‚Ä¢ {domain:<25}: {count:>6,} articles ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Plot domains\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_domains = domain_counts.head(15)\n",
    "    plt.barh(range(len(top_domains)), top_domains.values)\n",
    "    plt.yticks(range(len(top_domains)), top_domains.index)\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.title('Top Email Domains in Publisher Names')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No email addresses found in publisher names\")\n",
    "\n",
    "print(\"\\n5.4 PUBLISHER SPECIALIZATION ANALYSIS\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "def calculate_specialization(publisher_data):\n",
    "    \"\"\"Calculate how specialized a publisher is in specific stocks\"\"\"\n",
    "    if len(publisher_data) < 10:  # Need minimum articles for meaningful analysis\n",
    "        return 0, \"Insufficient data\"\n",
    "    \n",
    "    stock_coverage = publisher_data['stock'].value_counts()\n",
    "    top_stock_share = (stock_coverage.iloc[0] / len(publisher_data)) * 100 if len(stock_coverage) > 0 else 0\n",
    "    \n",
    "    if top_stock_share > 50:\n",
    "        specialization = \"Highly Specialized\"\n",
    "    elif top_stock_share > 25:\n",
    "        specialization = \"Moderately Specialized\"\n",
    "    else:\n",
    "        specialization = \"Diverse Coverage\"\n",
    "    \n",
    "    return top_stock_share, specialization\n",
    "\n",
    "print(\"Publisher Specialization Analysis (Top 15):\")\n",
    "print(\"-\" * 55)\n",
    "for publisher in publisher_counts.head(15).index:\n",
    "    publisher_data = news_df[news_df['publisher'] == publisher]\n",
    "    top_share, specialization = calculate_specialization(publisher_data)\n",
    "    \n",
    "    if top_share > 0:  # Only show publishers with sufficient data\n",
    "        print(f\"‚Ä¢ {publisher:<35}: {specialization:<25} (Top stock: {top_share:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. EXECUTIVE SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüéØ EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìà KEY FINDINGS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Data Overview\n",
    "date_range = f\"{news_df.index.min().strftime('%Y-%m-%d')} to {news_df.index.max().strftime('%Y-%m-%d')}\"\n",
    "print(f\"‚Ä¢ Data Period: {date_range}\")\n",
    "print(f\"‚Ä¢ Total Articles: {len(news_df):,}\")\n",
    "print(f\"‚Ä¢ Unique Publishers: {len(publisher_counts):,}\")\n",
    "\n",
    "# Most important insights\n",
    "most_common_topic = max(keyword_counts.items(), key=lambda x: x[1])[0].replace('_', ' ').title()\n",
    "busiest_day = day_counts.idxmax()\n",
    "busiest_hour_int = int(hour_counts.idxmax())  # Convert to integer\n",
    "top_publisher = publisher_counts.index[0]\n",
    "\n",
    "print(f\"‚Ä¢ Most Common Topic: {most_common_topic}\")\n",
    "print(f\"‚Ä¢ Busiest Publication Day: {busiest_day}\")\n",
    "print(f\"‚Ä¢ Peak Publication Hour: {busiest_hour_int:02d}:00\")\n",
    "print(f\"‚Ä¢ Most Prolific Publisher: {top_publisher}\")\n",
    "print(f\"‚Ä¢ Market Concentration: Top 10 publishers control {top_10_share:.1f}% of content\")\n",
    "\n",
    "# Trading implications\n",
    "print(f\"\\nüí° TRADING IMPLICATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"‚Ä¢ News volume peaks at {busiest_hour_int:02d}:00 - potential impact on market volatility\")\n",
    "print(f\"‚Ä¢ {busiest_day}s have highest news flow - prepare for increased activity\")\n",
    "print(f\"‚Ä¢ {most_common_topic} is most discussed - monitor related stocks closely\")\n",
    "print(f\"‚Ä¢ {top_publisher} dominates coverage - understand their reporting bias\")\n",
    "\n",
    "print(f\"\\nüìä DATA QUALITY ASSESSMENT:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"‚úÖ Date formatting complete\")\n",
    "print(f\"‚úÖ Temporal features extracted\")\n",
    "print(f\"‚úÖ Text analysis performed\")\n",
    "print(f\"‚úÖ Publisher analysis comprehensive\")\n",
    "print(f\"‚úÖ Time series patterns identified\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - ALL SECTIONS EXECUTED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 3: CORRELATION BETWEEN NEWS AND STOCK MOVEMENT\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 3: NEWS SENTIMENT & STOCK MOVEMENT CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA PREPARATION AND STOCK DATA LOADING\n",
    "# =============================================================================\n",
    "print(\"\\nüìÅ 1. LOADING AND PREPARING STOCK DATA...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create sample stock price data for demonstration\n",
    "def create_sample_stock_data(news_dates, symbol='AAPL'):\n",
    "    \"\"\"\n",
    "    Create realistic sample stock price data aligned with news dates\n",
    "    \"\"\"\n",
    "    # Get unique dates from news data\n",
    "    unique_dates = pd.Series(news_dates).dt.date.unique()\n",
    "    unique_dates = sorted(unique_dates)\n",
    "    \n",
    "    # Create stock data with some correlation to news sentiment\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    stock_data = []\n",
    "    base_price = 150.0\n",
    "    \n",
    "    for i, date in enumerate(unique_dates):\n",
    "        # Simulate some random movement with slight sentiment correlation\n",
    "        noise = np.random.normal(0, 0.02)  # Random noise\n",
    "        sentiment_effect = np.random.normal(0, 0.01)  # Will be replaced with actual sentiment\n",
    "        \n",
    "        price = base_price * (1 + noise + sentiment_effect)\n",
    "        volume = np.random.randint(1000000, 50000000)\n",
    "        \n",
    "        stock_data.append({\n",
    "            'date': pd.to_datetime(date),\n",
    "            'symbol': symbol,\n",
    "            'open': price * (1 + np.random.normal(0, 0.005)),\n",
    "            'high': price * (1 + abs(np.random.normal(0, 0.01))),\n",
    "            'low': price * (1 - abs(np.random.normal(0, 0.01))),\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        })\n",
    "        \n",
    "        base_price = price\n",
    "    \n",
    "    return pd.DataFrame(stock_data)\n",
    "\n",
    "# Create sample stock data aligned with news dates\n",
    "stock_df = create_sample_stock_data(news_df.index, 'AAPL')\n",
    "\n",
    "print(f\"Stock data shape: {stock_df.shape}\")\n",
    "print(f\"Stock data date range: {stock_df['date'].min()} to {stock_df['date'].max()}\")\n",
    "print(f\"Sample stock data:\")\n",
    "print(stock_df.head())\n",
    "\n",
    "# =============================================================================\n",
    "# 2. SENTIMENT ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüòä 2. PERFORMING SENTIMENT ANALYSIS ON HEADLINES...\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of text using TextBlob\n",
    "    Returns polarity score between -1 (negative) and 1 (positive)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analysis = TextBlob(str(text))\n",
    "        return analysis.sentiment.polarity\n",
    "    except:\n",
    "        return 0.0  # Neutral for errors\n",
    "\n",
    "print(\"Applying sentiment analysis to headlines...\")\n",
    "# Create a working copy and reset index to handle dates properly\n",
    "news_working = news_df.reset_index().copy()\n",
    "\n",
    "# Apply sentiment analysis\n",
    "news_working['sentiment'] = news_working['headline'].apply(analyze_sentiment)\n",
    "\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "sentiment_stats = news_working['sentiment'].describe()\n",
    "mean_sentiment = sentiment_stats['mean']\n",
    "print(f\"‚Ä¢ Mean Sentiment: {mean_sentiment:.4f}\")\n",
    "print(f\"‚Ä¢ Std Dev: {sentiment_stats['std']:.4f}\")\n",
    "print(f\"‚Ä¢ Min: {sentiment_stats['min']:.4f}\")\n",
    "print(f\"‚Ä¢ Max: {sentiment_stats['max']:.4f}\")\n",
    "print(f\"‚Ä¢ Positive (>0.1): {(news_working['sentiment'] > 0.1).sum():,} articles\")\n",
    "print(f\"‚Ä¢ Negative (<-0.1): {(news_working['sentiment'] < -0.1).sum():,} articles\")\n",
    "print(f\"‚Ä¢ Neutral: {((news_working['sentiment'] >= -0.1) & (news_working['sentiment'] <= 0.1)).sum():,} articles\")\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(news_working['sentiment'], bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "plt.axvline(mean_sentiment, color='red', linestyle='--', label=f'Mean: {mean_sentiment:.3f}')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of News Sentiment Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Categorize sentiments\n",
    "def categorize_sentiment(score):\n",
    "    if score > 0.1:\n",
    "        return 'Positive'\n",
    "    elif score < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "news_working['sentiment_category'] = news_working['sentiment'].apply(categorize_sentiment)\n",
    "sentiment_counts = news_working['sentiment_category'].value_counts()\n",
    "\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "        colors=['lightgreen', 'lightcoral', 'lightyellow'])\n",
    "plt.title('Sentiment Category Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DATE ALIGNMENT AND DAILY AGGREGATION\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüìÖ 3. DATE ALIGNMENT AND DAILY AGGREGATION...\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Extract date only from datetime (remove time component)\n",
    "news_working['date_only'] = pd.to_datetime(news_working['date'].dt.date)\n",
    "\n",
    "# Aggregate daily sentiment scores\n",
    "daily_sentiment = news_working.groupby('date_only').agg({\n",
    "    'sentiment': ['mean', 'std', 'count'],\n",
    "    'headline': 'count'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "daily_sentiment.columns = ['avg_sentiment', 'sentiment_std', 'unique_sentiments', 'article_count']\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "daily_sentiment.rename(columns={'date_only': 'date'}, inplace=True)\n",
    "\n",
    "print(\"Daily Sentiment Aggregation:\")\n",
    "print(f\"‚Ä¢ Total days with news: {len(daily_sentiment):,}\")\n",
    "print(f\"‚Ä¢ Average articles per day: {daily_sentiment['article_count'].mean():.1f}\")\n",
    "print(f\"‚Ä¢ Date range: {daily_sentiment['date'].min()} to {daily_sentiment['date'].max()}\")\n",
    "\n",
    "# Prepare stock data\n",
    "stock_df['date_only'] = pd.to_datetime(stock_df['date'].dt.date)\n",
    "daily_stock = stock_df.groupby('date_only').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "}).reset_index()\n",
    "daily_stock.rename(columns={'date_only': 'date'}, inplace=True)\n",
    "\n",
    "# Calculate daily returns\n",
    "daily_stock['daily_return'] = daily_stock['close'].pct_change() * 100\n",
    "daily_stock['daily_return'] = daily_stock['daily_return'].fillna(0)\n",
    "\n",
    "print(f\"\\nStock Data Summary:\")\n",
    "print(f\"‚Ä¢ Total trading days: {len(daily_stock):,}\")\n",
    "print(f\"‚Ä¢ Date range: {daily_stock['date'].min()} to {daily_stock['date'].max()}\")\n",
    "print(f\"‚Ä¢ Average daily return: {daily_stock['daily_return'].mean():.4f}%\")\n",
    "print(f\"‚Ä¢ Return volatility: {daily_stock['daily_return'].std():.4f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MERGE DATASETS\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüîÑ 4. MERGING NEWS SENTIMENT AND STOCK DATA...\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Merge on date\n",
    "merged_data = pd.merge(daily_sentiment, daily_stock, on='date', how='inner')\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_data.shape}\")\n",
    "print(f\"Common dates with both news and stock data: {len(merged_data):,}\")\n",
    "\n",
    "if len(merged_data) == 0:\n",
    "    print(\"‚ùå No common dates found! Check date alignment.\")\n",
    "else:\n",
    "    print(\"‚úÖ Successful merge! Proceeding with correlation analysis...\")\n",
    "    print(f\"Sample of merged data:\")\n",
    "    print(merged_data[['date', 'avg_sentiment', 'article_count', 'close', 'daily_return']].head())\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüìà 5. CORRELATION ANALYSIS...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if len(merged_data) > 0:\n",
    "    # Calculate correlation\n",
    "    correlation, p_value = pearsonr(merged_data['avg_sentiment'], merged_data['daily_return'])\n",
    "    \n",
    "    print(\"PEARSON CORRELATION RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"‚Ä¢ Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(f\"‚Ä¢ P-value: {p_value:.4f}\")\n",
    "    print(f\"‚Ä¢ R-squared: {correlation**2:.4f}\")\n",
    "    \n",
    "    # Interpret correlation strength\n",
    "    abs_corr = abs(correlation)\n",
    "    if abs_corr >= 0.7:\n",
    "        strength = \"Strong\"\n",
    "    elif abs_corr >= 0.4:\n",
    "        strength = \"Moderate\"\n",
    "    elif abs_corr >= 0.2:\n",
    "        strength = \"Weak\"\n",
    "    else:\n",
    "        strength = \"Very Weak\"\n",
    "    \n",
    "    print(f\"‚Ä¢ Strength: {strength}\")\n",
    "    \n",
    "    # Determine direction\n",
    "    if correlation > 0:\n",
    "        direction = \"Positive (Good news ‚Üí Positive returns)\"\n",
    "    else:\n",
    "        direction = \"Negative (Good news ‚Üí Negative returns)\"\n",
    "    \n",
    "    print(f\"‚Ä¢ Direction: {direction}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    if p_value < 0.05:\n",
    "        significance = \"Statistically Significant (p < 0.05)\"\n",
    "    else:\n",
    "        significance = \"Not Statistically Significant\"\n",
    "    \n",
    "    print(f\"‚Ä¢ Significance: {significance}\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # 6. VISUALIZATION\n",
    "    # =============================================================================\n",
    "    print(\"\\n\\nüìä 6. VISUALIZING CORRELATION...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    axes[0, 0].scatter(merged_data['avg_sentiment'], merged_data['daily_return'], \n",
    "                      alpha=0.6, color='blue', s=50)\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(merged_data['avg_sentiment'], merged_data['daily_return'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[0, 0].plot(merged_data['avg_sentiment'], p(merged_data['avg_sentiment']), \n",
    "                   \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Average Daily Sentiment Score')\n",
    "    axes[0, 0].set_ylabel('Daily Return (%)')\n",
    "    axes[0, 0].set_title(f'Sentiment vs Stock Returns\\nCorrelation: {correlation:.4f}')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Time series comparison\n",
    "    axes[0, 1].plot(merged_data['date'], merged_data['avg_sentiment'], \n",
    "                   label='Avg Sentiment', color='green', linewidth=1)\n",
    "    axes[0, 1].set_ylabel('Sentiment Score', color='green')\n",
    "    axes[0, 1].tick_params(axis='y', labelcolor='green')\n",
    "    axes[0, 1].set_title('Sentiment and Returns Over Time')\n",
    "    \n",
    "    ax2 = axes[0, 1].twinx()\n",
    "    ax2.plot(merged_data['date'], merged_data['daily_return'], \n",
    "            label='Daily Return', color='red', linewidth=1, alpha=0.7)\n",
    "    ax2.set_ylabel('Daily Return (%)', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # Sentiment distribution by return sign\n",
    "    merged_data['return_positive'] = merged_data['daily_return'] > 0\n",
    "    positive_days = merged_data[merged_data['return_positive']]\n",
    "    negative_days = merged_data[~merged_data['return_positive']]\n",
    "    \n",
    "    axes[1, 0].hist([positive_days['avg_sentiment'], negative_days['avg_sentiment']], \n",
    "                   bins=20, alpha=0.7, label=['Positive Returns', 'Negative Returns'],\n",
    "                   color=['green', 'red'])\n",
    "    axes[1, 0].set_xlabel('Sentiment Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Sentiment Distribution by Return Type')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Lag correlation analysis\n",
    "    lags = range(0, 6)  # 0 to 5 day lags\n",
    "    lag_correlations = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        if lag == 0:\n",
    "            sent = merged_data['avg_sentiment']\n",
    "            ret = merged_data['daily_return']\n",
    "        else:\n",
    "            sent = merged_data['avg_sentiment'].iloc[:-lag]\n",
    "            ret = merged_data['daily_return'].iloc[lag:]\n",
    "        \n",
    "        if len(sent) > 10 and len(ret) > 10:  # Minimum data points\n",
    "            corr, _ = pearsonr(sent, ret)\n",
    "            lag_correlations.append(corr)\n",
    "        else:\n",
    "            lag_correlations.append(np.nan)\n",
    "    \n",
    "    axes[1, 1].bar(lags, lag_correlations, color='purple', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Lag (Days)')\n",
    "    axes[1, 1].set_ylabel('Correlation Coefficient')\n",
    "    axes[1, 1].set_title('Correlation at Different Lags')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 7. ADVANCED ANALYSIS\n",
    "    # =============================================================================\n",
    "    print(\"\\n\\nüîç 7. ADVANCED ANALYSIS...\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Analyze extreme sentiment days\n",
    "    high_sentiment = merged_data.nlargest(10, 'avg_sentiment')\n",
    "    low_sentiment = merged_data.nsmallest(10, 'avg_sentiment')\n",
    "    \n",
    "    print(\"Top 10 Most Positive Sentiment Days:\")\n",
    "    print(\"Date\\t\\tSentiment\\tReturn\")\n",
    "    for _, row in high_sentiment.iterrows():\n",
    "        print(f\"{row['date'].strftime('%Y-%m-%d')}\\t{row['avg_sentiment']:.4f}\\t\\t{row['daily_return']:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nAverage return on high sentiment days: {high_sentiment['daily_return'].mean():.2f}%\")\n",
    "    print(f\"Average return on low sentiment days: {low_sentiment['daily_return'].mean():.2f}%\")\n",
    "    \n",
    "    # Volume vs Sentiment analysis\n",
    "    volume_sentiment_corr, volume_p_value = pearsonr(merged_data['avg_sentiment'], merged_data['volume'])\n",
    "    print(f\"\\nVolume-Sentiment Correlation: {volume_sentiment_corr:.4f} (p-value: {volume_p_value:.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. GIT WORKFLOW FOR TASK 3\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüîß 8. GIT WORKFLOW INSTRUCTIONS FOR TASK 3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9. EXECUTIVE SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\\nüéØ TASK 3 EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(merged_data) > 0:\n",
    "    print(\"\\nüìà KEY FINDINGS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"‚Ä¢ Dataset: {len(merged_data):,} days with both news and stock data\")\n",
    "    print(f\"‚Ä¢ Sentiment-Return Correlation: {correlation:.4f} ({strength})\")\n",
    "    print(f\"‚Ä¢ Statistical Significance: {significance}\")\n",
    "    print(f\"‚Ä¢ Direction: {direction}\")\n",
    "    print(f\"‚Ä¢ Explained Variance (R¬≤): {correlation**2:.4f}\")\n",
    "    \n",
    "    print(\"\\nüí° TRADING IMPLICATIONS:\")\n",
    "    print(\"-\" * 35)\n",
    "    if correlation > 0.1 and p_value < 0.05:\n",
    "        print(\"‚úÖ Significant positive correlation found\")\n",
    "        print(\"‚Üí Consider sentiment as one factor in trading decisions\")\n",
    "        print(\"‚Üí Monitor news sentiment for potential market movements\")\n",
    "    elif correlation < -0.1 and p_value < 0.05:\n",
    "        print(\"‚úÖ Significant negative correlation found\")\n",
    "        print(\"‚Üí Market may react contrarily to news sentiment\")\n",
    "        print(\"‚Üí Consider contrarian strategies\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Weak or insignificant correlation detected\")\n",
    "        print(\"‚Üí News sentiment alone may not be reliable predictor\")\n",
    "        print(\"‚Üí Combine with other technical/fundamental factors\")\n",
    "    \n",
    "    print(\"\\nüîß METHODOLOGY:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"‚úì TextBlob for sentiment analysis\")\n",
    "    print(\"‚úì Pearson correlation for linear relationship\")\n",
    "    print(\"‚úì Daily aggregation and date alignment\")\n",
    "    print(\"‚úì Multiple visualization approaches\")\n",
    "    print(\"‚úì Lag correlation analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ANALYSIS INCOMPLETE: No overlapping data found\")\n",
    "    print(\"Please ensure proper date alignment between news and stock datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 3 COMPLETE - SENTIMENT CORRELATION ANALYSIS FINISHED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d32da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
